# 大模型微调

大模型微调(finetune)是让已有大模型获得新知识或强化特定知识,以完成特定工作的技术.打个比方来说,我们从小学到大学学的都是通识教育(即便大学也是专业内的通识教育),这就好比一般的大模型;而微调就像是找到工作后的岗前培训,学的是岗位上真正用到的专项知识.

实际上我们一般用到的llm命名中会有`code`,`math`,`instruct`字段,这些已经是finetune过的了,没有finetune过的一般命名中会带`base`字样,这种是没有对话能力的.

在用法方面,llm微调往往用在专业化的细分领域,比如我们可以用微调技术让通用模型比如`qwen2.5`学会更多的法律知识以可以作为法律助手为我们提供法律援助.你可能会说这种功能靠外部知识库做rag也能做到,没错,但微调有它的额外优势--知识是内化的,llm不光学到了知识,还更容易用这些学会的知识进行思考.微调过的llm通常也是结合rag一起使用,会搭配专用知识库做外部知识储备.

## 微调方式

目前的微调技术大致是3种形式

+ full,即全量微调,用数据集训练直接修改模型上的权重层

+ freeze,即冻结大部分权重层,仅通过训练修改未冻结的权重层

+ lora,冻结全部权重层,构造一个小规模参数的旁路网络和原网络一起参与训练和推理,训练这个旁路网络以达到微调的效果.而lora又有很多变种
    + LoRA,即最原始的lora,
    + rsLoRA,LoRA 通过添加低秩适配器进行微调然而lora_rank的增大往往会导致梯度塌陷，使得训练变得不稳定.这使得在使用较大的lora_rank进行LoRA 微调时较难取得令人满意的效果.rsLoRA(Rank-Stabilized LoRA)通过修改缩放因子使得模型训练更加稳定
    + DoRA,DoRA将权重矩阵分解为大小与单位方向矩阵的乘积,并进一步微调二者(一个旁路矩阵变成两个旁路矩阵,对方向矩阵则进一步使用 LoRA 分解),从而实现 LoRA 与 Full Fine-tuning之间的平衡.
    + PiSSA,在LoRA中,适配器矩阵 A 由 kaiming_uniform 初始化,而适配器矩阵 B 则全初始化为0.这导致一开始的输入并不会改变模型输出并且使得梯度较小,收敛较慢. PiSSA通过奇异值分解直接分解原权重矩阵进行初始化，其优势在于它可以更快更好地收敛.
    + QLoRA,即量化lora,用量化技术进一步减小训练和推理消耗一般有Q4,Q8,Q16三种量化方式

通常在消费级设备上可以搞搞的就是lora,本文也将以lora为主要介绍内容

## 微调工具

现在的微调工具也算百花齐放,我个人还是推荐[LLaMA Factory](https://github.com/hiyouga/LLaMA-Factory/blob/main/README_zh.md),文档比较清晰而且更新挺勤的,关键amd也能用.如果你是N卡也可以用[unsloth](https://github.com/unslothai/unsloth).

