# 大语言模型

大语言模型应该说是这次AIGC技术爆发的起爆点.ChatGPT的横空出世确实让人印象深刻.随之而来每一次模型迭代也确实在性能上有所改进.

但大语言模型即便是公认成本大幅降低的deepseek出现的现在依然可以认为是大公司才能玩的技术,它也仅仅是让门槛从超级巨头下降到了一般大公司而已.对于个人用户,全量大语言模型依然是玩不起的存在,我们能本地折腾的还是经过向量化蒸馏后的小参数残废版本.

虽说是残废版本,但也不是不能玩.只是与全量版本相比人味少点,智力差点,仿佛是大学生和小学生的区别.但它免费而且也还是有一定能力,而且随着技术的发展相信小参数的模型也一定会成长的越来越强.本着战未来的出发点我们也还是有必要了解下的.

## 原理

总的来说,目前llm的原理还是黑盒,没人知道为什么它就能忽然有了类似"智力"的东西,但我们还是可以用一句话来概括它的原理--量变引起质变.

至于内部结构具体是啥样,训练数据,训练过程究竟什么样,各家都多少有点不同,但都可以总结为通过各种花式堆叠大量的Transformer构造模型,然后用无监督学习的方式训练这个模型,最后再根据需要做有监督的finetune出需要的模型,这样一个llm就炼好了

虽然大模型的开发厂商八仙过海各显神通,但总体还是GPT的扩展即自回归模型为主,自回归模型可以简单理解为单向Transformer的结构从左往右学习的模型,只能利用上文或者下文的信息,根据前文预测后文.而后续各家出的大模型基本沿用这一结构,然后在上面进行自己的创新.

具体的几个经典大模型的结构我们在后面介绍如何微调前再说.

## 出圈点

其实大语言模型出圈并不是出在性能多强大,而是交互方式上,或者说是在微调部分.最早大语言模型的用法是拿无监督学习训练出来的基模作为基础,有什么任务就做针对性的微调.这个时代的王者就是谷歌的Bert.显然这种方式的使用门槛是比较高的.而Chat-GPT的出圈主要是它被微调成了一个对话模型.然后一般大众忽然发现它真的可以对话,说的话似乎挺有智力的,这才出的圈,就技术上来说,其实底层技术并没有什么根本性的变化,只是产品形态更合适了,模型规模也让它性能达标了而已.

## 什么决定LLM的能力

根据传统的训练`Scaling Laws`模型训练的`loss`直接和训练的计算量相关(包括模型参数大小,训练的数据量),当`loss`降低到一定程度模型会出现[涌现行为](https://zhuanlan.zhihu.com/p/621438653),即在任务中的准确性有突然的提升.

我们可以直观理解为"脑子越大,书读越多,成绩越好",并会在某个时刻突然"开窍"了.以`Qwen`为例,`Qwen-1.5-72B`训练了`3T tokens`;`Qwen-2.5-72B`训练了`18T tokens`,后者效果自然会更好.

`Qwen-32B`一定会显著好于`Qwen-7B`,但是再往上`Qwen-72B`对于特定应用场景的相对提升可能变小了.

我们大致可以根据模型规模预判一个模型的大致能力

+ 小规模(~<5B): 典型如`Qwen2.5-Coder-3B`,可以胜任基本的语言理解,摘要总结,翻译等入门级别任务.
+ 中等规模(~10B): 典型大小`7B,14B`,可以胜任简单的编程任务和逻辑推理任务,复杂的就不行了
+ 中上规模(~30B): 典型如`Qwen-32B-Coder`,已经能接近市面最好的模型的编程能力了,逻辑推理也比中等规模有显著增强,而且Q4量化之后在`16GB`左右,也算是接近消费级设备上限的甜品尺寸.但一样的不要对它有过多期待.
+ 旗舰型(>70B): 基本是各家最好的模型,不过注意有一类特殊的`MoE`模型,在比较性能的时候通常用激活参数,比如`DeepSeek-V2`,激活参数`21B`,但是全部参数是`236B`,然而推理一般是需要全部加载到内存中的,实际上很难跑到消费级设备上.

当然也有很多的benchmark,但往往benchmark是公开的,很容易就可以被所谓"未来数据"影响从而提高得分.所以评估模型的能力最好还是自己下下来用用.

事实上我们依然需要跳出舆论的影响,对大模型的能力有充分客观的认知:

+ 如果结果的准确性无法被轻易验证,那么使用`LLM`就毫无意义.LLM 会产生幻觉(hallucination),这也让它们变得并非绝对可靠.很多时候如果你能够验证LLM的输出是否正确的话,你其实也就没必要用它了,这也是它尴尬的地方.

+ LLM 都是`金鱼脑袋`,也就是说较短的上下文长度限制了它们的发挥.虽然有些模型使用了更大的上下文长度来训练.但是其有效上下文长度通常小的多.实际上,一个LLM一次只能记住相当于一本书里几章的内容,如果是代码的话则是`2000`到`3000`行(因为代码的token密集度更高).当然也可以通过微调或者使用检索增强生成这类的工具来尝试改善,但是只能说收效甚微.

+ 不要指望LLM写代码的能力.往好了说它们的写码能力也只不过是一个读过大量文档的本科生的水平.让llm写点简单组件还行,涉及到细节,业务的就不行了.如果要让他们做这方面工作,你需要替他们思考--将逻辑业务逻辑拆分成小段让llm来编写,你则负责debug和整合.当然这可以替你省不少写八股文的时间,让你可以花更多时间在在对业务做建模等更宏观的事情上,但并不能降低你的心智负担,整体开发时间也不会少多少.而且在编程语言上llm是严重偏科的,毕竟训练语料就偏,相对质量比较高的是`python`,`js`,`sql`,而像`C`,`C++`这样的基本就不太行了.

+ 不要指望llm的逻辑推理能力,包括数学能力.类似代码能力,逻辑推理能力也不是llm的强项,而且可能比写代码还差的多.而且也是任务越小效果越好.所以那些完全依赖llm作判断触发函数执行特定任务的宣传看看就好.当然不是说agent现在还实现不了,其实还是一样的方法,你把任务拆细,把没那么关键的任务交给llm控制执行其实问题不太大,只是你最好还是在他做好后check一下.

那目前哪些任务比较适合给llm干呢?

+ 校对类工作,比如给一篇文章让他提出修改意见,帮忙润色,比如给一段代码让它写注释,提出改进意见.
+ 翻译,其实原理和校对类工作类似.llm的翻译连贯性很不错,但准确性需要自己把下关.
+ 提供创意,你可以和他讨论不断修改,然后再一点点扩充,以丰富这个创意.比如给llm一个大致方向让它发挥写个故事大纲,然后慢慢讨论整个剧本.
