# LLM的推理

通常我们说的用大模型指的就是llm推理,而推理又可以分为两种

+ 在线推理服务,现在大部分的大科技公司,相关创业公司都提供了在线推理服务,价钱有便宜有贵,服务有稳定有不稳定.同时往往还提供api,我们可以在本地使用api对接客户端来做更多的事
+ 本地推理,很多模型是开源的,我们可以下载模型的权重文件然后利用本地的gpu,cpu等算力设备用一些软件自己做推理.

下面是我总结的两种方式的利弊对比

| 对比项目     | 在线推理服务                                  | 本地推理                                        |
| ------------ | --------------------------------------------- | ----------------------------------------------- |
| 智力水平     | 可以使用更大参数量的模型,通常智力更高         | 受限于硬件只能使用智力较低的小参数版本模型      |
| 对话自由程度 | 有审核,敏感话题会被拦截                       | 通常默认版本也有敏感话题拦截,但通常都有办法绕开 |
| 使用限制     | 受限于冲的钱,以及一些其他拦截策略,相对受限    | 随便回答                                        |
| 推理速度     | 通常不太快                                    | 可以选择快但智力低的也可以选择慢但智力高的      |
| 成本         | 就api调用的费用,用的少就成本低,用的多就成本高 | 硬件有一定门槛,但可以用来干别的,长期看成本低    |
| 微调         | 无法微调                                      | 可以微调                                        |

个人更加推荐根据需求选择推理方式--如果你有一些需要快速响应的任务比如简单翻译,可以本地部署一个2b左右的模型,通常表现就已经足够好;对于一般需求,比如做个总结归纳什么的,辅助你写写文章什么的,你可以本地部署7b左右的模型就已经够用了,而这个规模的模型稍微好点的核显都能跑的挺好;而一些需要更高智能水平的任务可以交给在线推理服务,选个全量的deepseek模型放在那边让它慢慢想,想好了你再来看结果就好



## 在线推理服务

比较主流的在线推理服务供应商包括

+ [阿里百炼](https://tongyi.aliyun.com/qianwen/),提供qwen系列,deepseek模型等主流开源模型
+ [deepseek](https://www.deepseek.com/),提供deepseek本家模型,不过服务相当不稳定
+ [openai](https://chatgpt.com),提供chatgpt本家模型.

二线厂商也包括

+ [硅基流动](https://siliconflow.cn/zh-cn/),类似阿里百炼,提供主流开源模型
+ [智谱](https://chatglm.cn/main/guest?lang=zh),提供chat-glm系列模型
+ [字节豆包](https://www.volcengine.com/product/doubao),提供豆包系列大模型

如果要我推荐我还是推荐阿里百炼或者openai,他们算是相对服务稳定的供应商了.注册,充钱,拿到api key,然后下载一个cherry studio设置一下就能用了.

## 本地推理

本地推理相对就更复杂些,我们需要根据硬件和需求在性能和推理速度上做权衡,同时如果需求复杂我们可能还得写程序自行封装.

### 什么决定模型的推理速度

模型的推理速度受硬件算力水平,显存带宽,预填充参数规模以及量化方式影响.要想准确分析自己的模型的推理速度需要先了解大模型的推理流程

> llm的推理流程

我们都知道当今的语言大模型实质上的架构都基本一致,是仅有解码器的`Transformer+`.如果你在网上搜什么是`Transformers`很可能会搜到[Jay Alammar 这篇著名的博客](https://jalammar.github.io/illustrated-transformer/).对用户来说这些细节能帮助我们更深入了解内部原理但是并不重要.

大模型本质上是一种特殊的程序:输入指令,往后吐字.模型并不能直接处理文字,现在比较通用的做法是使用`tokenizer`把文本映射到词表中的坐标--可以想象成一本字典,在里面的所有的词或字用从0开始的序号标记,模型编码的时候就是从字典里面读这个的标号,解码的时候反之.下图我们简化一下,每个`token`对应一个字:

```text
      我是
      ^ ^
-------------------
|        LLM      |
-------------------
^ ^ ^ ^ ^ ^
你是谁 ? 我是
```

没错,实际上llm的推理就是填字的操作.用户输入prompt:`你是谁`,模型经过一系列计算,在所有可选词中选取可能性最大的`我`进行输出,然后输出的`我`又再作为输入往后推下一个字得到`是`以此类推直到推理出结束符号后停止推理.

注意上面是理想模型,一般情况下我们不会只选取概率最大的`token`进行输出,这样这个程序的输出变成完全确定,并不利于创意类生成,所以需要额外的参数(`temperature`,`topk`等等)来加入一些采样随机性.

我们可以区分实际计算的两个过程:

1. 预填充(`prefill`):即模型在理解一长段的话.模型在计算的同时需要保存一部分后面生成所需要的中间状态(KV Cache),毕竟每次循环都会用到之前的数据,没有必要反复计算
2. 自回归解码(`decoding`):对应迭代生成的过程,模型一步步,token by token的生成结果.

我们在使用流式的大模型应用时,输入请求之后会等一会儿(`prefill`),模型开始出第一个词然后继续输出(`decoding`),也就是对应这两个状态.

而在llm内部,其结构大致可以理解为

```text
    token
      ^
=================== LLM
   unembbeding
-------------------
       ^ ---向量
 transformer blocks
     ....
       ^ ---向量
-------------------
   embbeding
=================== llm使用
       ^
     token
```

基于`Transformers`的语言模型有着非常规整的结构--除了输入输出就是层层叠叠重复的`Transformer blocks`.
每个模型的架构确定,则每一步的计算公式都确定了,例如线性的`f(x)=ax+b`,而模型的权重或者参数即其中的`a`和`b`.
实际上 LLM 的参数量和计算量都要大非常多,以`10B`参数为例,一共有`10^10`个参数,如果都用半精度(FP16/BF16)存储,大小为`10^10 * 16/8 bytes = 20 GB`.

#### 机器怎么做推理

上面是算法层面流程,机器又是怎么执行这些算法的呢?我们以我们的核显作为基准,假设模型可以完整加载到显存来看看

1. 把模型参数加载到显存中
2. 小部分计算参数搬运到GPU核心中参与计算并返回

```text
|-----------------------------|
| 模型文件 | 硬盘 |
| -------- |
|          |
 载入(ssd硬盘读速度) 
    |
    v
|-------------------------------------------|
|  模型文件                    本地推理              |   Vram/GTT
|    |                                      |
|    v                                      |
| 反量化并取出要计算的参数部分(cpu指令速度,GPU规模) |
| ------------------------------------------------- |
| ^                                                 |
| 载入(显存速度)                                    |
| 返回结果(显存速度)                                |
| v                                                 |
| ---------------------------------                 |
| 计算(GPU算子速度)                                 | iGPU |
| ---------------------------------                 |
```

那就可以看出来了,模型推理大致可以分成两段时间

1. 模型加载到显存中的时间
2. 模型参数从显存中取出计算后返回的时间

其中第一段一般可以预加载,即在做推理之前就先将模型加载到显存,因此第一段用户一般感知不到.而第二段就是实打实的用户必然能感知到的推理速度了.

那由这个流程可知,影响到推理速度的因素有

1. 拆分模型参数时的cpu指令速度,这个一般可以忽略不计
2. 显存速度,这个影响计算参数的加载和返回结果的吐出速度
3. GPU规模,影响每次可以计算的规模即吞吐量.
4. GPU算子速度.这个影响计算的速度.

如果显存不够放下全部大模型的参数,在llama.cpp中就会预先拆分一部分给CPU跑.流程和上面GPU的类似,但注意这个过程是串行的不是并行的,也就是说先跑完GPU的部分才会再跑CPU的部分.这就会大大拖慢推理速度.

但无论是全在GPU中跑还是拆分了跑,从llm的推理流程来说其实每个步骤都是在跑llm完整模型,这个时间其实都是一致的.

#### 量化对推理速度的影响

llm在gpu/cpu中推理时往往使用的是fp16精度,而量化说白了就是在保存模型参数时根据特定的规则降低精度从而让模型在权重量不变的情况下尺寸显著减小.也就是说模型的量化可以理解为是一种有损压缩技术.

从这个角度理解我们量化过的模型在使用时也就需要解压让权重值重新变回fp16精度(即Dequantize),这无疑会增加操作.因此量化操作往往会拖慢推理速度.但也有例外,

+ 一些向量化方法可以减少kvcache规模,这样decode时候的访存速度就会有所提高.这样可能反而可以提高推理速度.

#### 推理框架对推理速度的影响

llm当然可以直接用pytorch跑,但显然效率会很低.我们通常做推理都是将模型部署在llm的推理框架上执行的.llm的推理框架通常都是针对特定场景专门做优化的,一般来说只有最合适没有最好.它们的性能表现很可能在不同设备上相差很大.下面是我总结的不同场景下比较主流的推理框架

+ `llama.cpp`,专为端侧推理优化的推理框架,也是现在的绝对主流,它必须结合GGUF格式的量化模型使用,可能不是最快的,但绝对是生态最好的.它的优势是在CPU推理上,在没有独显的情况下`llama.cpp`可以单靠CPU推理,且推理速度还算不错,而且还可以CPU/GPU混合推理,充分利用本地资源.另外,`llama.cpp`对非fp16的量化过的模型推理有优化,实测可以有1.5~3倍的速度提升
+ `vllm`,为服务端部署大模型进行优化的框架,它的特色是支持较高并发,并且支持多节点并行推理

+ `ktransformers`,一个性能优先的推理框架,它会用最先进最新的优化技术优化推理速度,通常用英伟达GPU的情况下相同条件会比`llama.cpp`快上一倍以上

本文将会以以`llama.cpp`为基础的`ollama`作为推理的基准框架.

### 如何评估推理速度

前面我们提到推理的时候分为两个阶段,对应可以有两类指标评价:

+ 预填充(`prefill`)阶段: 固定上下文长度下,第一个生成词出来的时间(Time to Fist Token, TTFT),单位`ms`;也可以用每秒处理多少个 token 的速度定义(Prompt Processing,PP),单位`tokens/s`
+ 自回归解码(`decoding`)阶段:固定上下文长度,每个输出词的平均耗时(Time per Output Token, TPOP),单位`ms`;也可以用文本生成的速度定义(Text Generage,TG),单位也是`tokens/s`.

我们可以大致理解预填充阶段是大模型的启动阶段,它会大致分析下要说的内容;而自回归解码阶段就是模型边想边说的阶段,这个阶段它会组织语言一个字一个字的吐出结果.在表现形式上预填充阶段往往会给个等待图标一直转圈圈,到自回归解码阶段时则是流式的吐出文本来.那体验上来说显然自回归解码阶段的速度更加重要,要是字吐的太慢低于正常人的阅读速度(10个字/s)显然体验就很差了.

因此我个人评估本地推理的速度基本就以`10 tokens/s`为界限,低于这个值基本就可以认定为无法对话.

当然如果你只是要它出结果,并不是和它对话,那慢也不是不能接受

### 找到推理的硬件瓶颈

在PC游戏和装机领域我们经常会听到CPU瓶颈,显卡瓶颈对游戏性能的影响.同样对于`LLM推理`,在大多数情况下对于GPU推理来说有这样的结论--在预填充阶段主要是算力瓶颈(GPU规模+GPU算子速度),`Transformer`并行处理`prompt token`需要大量的计算;在自回归解码阶段主要是带宽瓶颈,计算不够密集导致参数还没搬过来只能空等.而对于用户来说,体感影响最大的显然是自回归解码阶段的速度.

对于更加严肃准确一点的推理入门知识可以参考[A guide to LLM inference and performance | Baseten Blog](https://www.baseten.co/blog/llm-transformer-inference-guide/).

对于硬件来说我们可以定义一个`ops/byte`的性能指标,即搬运多少数据时可以做多少计算,来查看是算力不够还是带宽不够.注意在LLM推理中无论模型是否量化,计算都应该在`FP16`或者更高的精度.所以我们在衡量算力的时候应该用FP16的指标,比如`4060Ti`的`FP16`算力是`22.06 TFLOPS`(每秒执行浮点数计算的次数).这个算力除以内存或者显存带宽就可以算出相应指标，比如 4060Ti 的显存带宽是`288GB/s`,即`76.6 ops/byte`。但是对于专用的推理卡比如`A10`,`FP16`算力为`125 TFLOPS`,24GB GDDR6的显存带宽达到`600 GB/s`则能达到`208 ops/byte`.而我们的780m不超频的情况下`FP16`算力是`16.59 TFLOPS`,而显存(即内存)在双通道DDR5 5600下最大为`30GB/s`.该性能指标也就`553 ops/byte`.

当然这个值并不是越高就越说明性能强劲,它反应的是带宽是否成为推理的瓶颈,比如当上下文长度为`4096`时,Llama 2注意力部分的计算密度为`62 ops/byte`,那说明以上例子中的3款显卡显存(内存)带宽都不是瓶颈,那瓶颈不在显存带宽上自然也就在算子速度上了,这个指标越偏离目标计算密度说明带宽颈越大.由此可见`4060Ti`最为均衡考虑到价格还真是推理神卡,而780m显然算力有很大的带宽.我们可以通过超频到3200大致获得10%左右的算力性能提升,而通过超频内存到8400可以让内存带宽达到50GB/s,此时带宽瓶颈依然存在,但显然已经缓解很多了.

### 模型选择

显然能选择模型的范围在我们的硬件已经确定的情况下就已经固定了.固然显存是制约我们使用大规模模型的,推理速度也让我们不会选择大规模的模型,下面是我总结的主流显存下比较合适的模型,规模和量化组合

#### 12/16G显存显卡

> 快速响应模型,用于简单翻译,写写注释这类需要快速响应的需求

+ `qwen2.5:3b-instruct-q5_K_M`,做做简单翻译是可以的
+ `deepseek-r1:1.5b-qwen-distill-q8_0`,帮助写写注释还可以

> 对话模型,用于问答式的处理问题

+ `deepseek-r1:7b`/`deepseek-r1:7b-qwen-distill-q4_K_M`,中文环境下使用
+ `deepseek-r1:8b`/`deepseek-r1:8b-llama-distill-q4_K_M`,英文环境下使用

> 离线处理模型,用于时效性要求不高需要深入思考后得出结论的模型

+ `deepseek-r1:14b`/`deepseek-r1:4b-qwen-distill-q4_K_M`,离线分析使用
+ `qwen2.5-coder:14b-instruct-q5_K_M`,生成代码时使用

#### 8G显存显卡

> 快速响应模型,用于简单翻译,写写注释这类需要快速响应的需求

+ `qwen2.5:3b-instruct-q5_K_M`,做做简单翻译是可以的
+ `deepseek-r1:1.5b-qwen-distill-q8_0`,帮助写写注释还可以

> 对话模型,用于问答式的处理问题

+ `deepseek-r1:7b`/`deepseek-r1:7b-qwen-distill-q4_K_M`,中文环境下使用
+ `deepseek-r1:8b`/`deepseek-r1:8b-llama-distill-q4_K_M`,英文环境下使用

> 8G

+ qwen2.5:3b-instruct-fp16 小模型,没啥废话
+ deepseek-r1:7b-qwen-distill-q4_K_M 中模型,中文环境下使用
+ deepseek-r1:8b-llama-distill-q4_K_M 中模型,英文环境下使用
+ qwen2.5-coder:7b-instruct-q5_K_M 中模型,编程用

正常消费级显卡能本地部署的也就这个程度了,如果你有22G+显存的高端显卡可以尝试将大模型替换到32b这个规模,但个人认为还是大可不必.理性认识到本地大模型的能力边界并针对性的制定任务远比砸钱堆硬件然后失望而归体验好的多.