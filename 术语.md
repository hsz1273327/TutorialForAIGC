# 术语

AIGC作为一个相对比较专业的领域自然有自己的黑话(专业术语),下面是整理的常见专业术语方便查阅理解:

+ AGI 人工通用智能

    也称为强人工智能,它是指一种具备像人类一样的学习,推理,思考,决策和自我发展等能力的人工智能系统.与目前的人工智能系统相比AGI具有更广泛的适用性和更高的智能水平,能够处理各种复杂的任务和问题,并能够持续地自我学习和进化,从而实现真正的人类水平智能.目前它只能算是一个愿景,还没有任何一个人工智能模型能够达到真正的AGI水平.

+ Transformer
    来自谷歌著名的论文[attention is all you need](https://arxiv.org/pdf/1706.03762)中提出的一种基于注意力机制(attention mechanism)的模型结构.
    传统的序列到序列模型(如循环神经网络RNN)存在着信息传递效率低,难以并行计算等问题,而Transformer通过引入自注意力机制来解决这些问题.自注意力机制允许模型根据序列中的其他位置调整每个位置的表示,从而提高模型对上下文的理解能力.与RNN相比Transformer能够更好地处理长序列,并且可以使用并行计算来提高训练速度.
    事实上Transformer并非唯一解,我们可以看到很多其他尝试,但至少目前它以及它的变体是主流

+ Attention mechanism 注意力机制
    是一种用于机器学习和自然语言处理的技术,它可以根据输入的信息动态地将注意力集中在不同的位置,从而使得模型能够更好地理解和处理输入的序列数据.注意力结构按照输入的来源可以分为自注意力,多头注意力等多种变体

+ sequence-to-sequence序列到序列

    是一种常见的神经网络架构,用于将一个序列映射到另一个序列.它由两个主要组件组成--编码器和解码器.编码器将输入序列转换为一个向量表示;解码器则将这个向量解码成目标序列.在这个过程中模型会学习到输入序列和输出序列之间的对应关系从而实现转换.

+ In-Context Learning上下文理解
    指机器学习模型可以根据上下文信息对相同的词汇在不同上下文中进行不同的理解和处理,以更准确地对文本进行理解和生成.

+ Few Shot
    在给定非常少的样本(通常是几个或者十几个)的情况下让模型学会针对该领域的语言理解或生成任务.具体来说就是通过给模型提供一些示例让模型在学习这些示例的基础上能够根据给定的提示或问题进行推理或生成相应的文本.

+ Zero Shot
    是指在没有接受任何训练的情况下让模型对没有出现在训练集中的任务进行推理或生成相应的文本.具体来说就是通过给模型提供一些与目标任务相关的信息,例如一些关键词或描述,让模型能够推断出目标任务所需要的信息并生成相应的文本.

    Few Shot,Zero Shot这两个概念主要是为了描述语言模型的泛化能力,也就是模型在学习过一些示例后能否推广到新的任务或问题.在实际应用中这种泛化能力非常重要,因为很难为每个任务或问题都提供大量的样本进行训练.这两个方法通常适合拥有较大数据集的模型,比如GPT

+ Fine tuning 微调
    在预训练模型的基础上使用标注数据进行微调以适应特定任务.

+ Prompt
    Prompt是一种基于自然语言生成模型的输入提示机制,可以在一定程度上指导模型的生成结果.通过Prompt我们可以给模型提供一些提示,条件或者上下文信息,以期望模型能够生成更准确更符合预期的输出结果